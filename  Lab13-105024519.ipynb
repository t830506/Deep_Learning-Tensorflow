{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what I did:  \n",
    "- Use other pretrained neural networks to generate stylized images.  \n",
    "- Try changing the weights for the style, content, and denoising.  \n",
    "- Use different weights for the content and style layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unahsu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipyd\n",
    "import scipy.misc\n",
    "from libs import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "content_directory = 'contents/'\n",
    "style_directory = 'styles/'\n",
    "\n",
    "# This is the directory to store the final stylized images\n",
    "output_directory = 'image_output/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "# This is the directory to store the half-done images during the training.\n",
    "checkpoint_directory = 'checkpoint_output/'\n",
    "if not os.path.exists(checkpoint_directory):\n",
    "    os.makedirs(checkpoint_directory)\n",
    "    \n",
    "content_path = os.path.join(content_directory, 'wonder-woman.jpg')\n",
    "style_path = os.path.join(style_directory, 'starry-night.jpg')\n",
    "output_path = os.path.join(output_directory, 'wonder-woman-starry-night-iteration-1000-vgg16.jpg')\n",
    "\n",
    "# please notice that the checkpoint_images_path has to contain %s in the file_name\n",
    "checkpoint_path = os.path.join(checkpoint_directory, 'wonder-woman-starry-night-iteration-1000-vgg16-%s.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_image = utils.imread(content_path)\n",
    "\n",
    "# You can pass several style images as a list, but let's use just one for now.\n",
    "style_images = [utils.imread(style_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use other pretrained neural networks to generate stylized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "VGG16_LAYERS = (\n",
    "    'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "    'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "\n",
    "    'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "    'relu3_3', 'pool3',\n",
    "\n",
    "    'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "    'relu4_3', 'pool4',\n",
    "\n",
    "    'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "    'relu5_3',\n",
    ")\n",
    "\n",
    "\n",
    "def net_preloaded(input_image, pooling):\n",
    "    data_dict = np.load('libs/vgg16.npy', encoding='latin1').item()\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(VGG16_LAYERS):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels = get_conv_filter(data_dict, name)\n",
    "            # kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "\n",
    "            bias = get_bias(data_dict, name)\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "\n",
    "            # bias = bias.reshape(-1)\n",
    "            current = conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = pool_layer(current, pooling)\n",
    "\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(VGG16_LAYERS)\n",
    "    return net\n",
    "\n",
    "def conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, weights, strides=(1, 1, 1, 1), padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def pool_layer(input, pooling):\n",
    "    if pooling == 'avg':\n",
    "        return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                              padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                              padding='SAME')\n",
    "\n",
    "# before we feed the image into the network, we preprocess it by \n",
    "# extracting the mean_pixel from it.\n",
    "def preprocess(image):\n",
    "    return image - VGG_MEAN\n",
    "\n",
    "# remember to unprocess it before you plot it out and save it.\n",
    "def unprocess(image):\n",
    "    return image + VGG_MEAN\n",
    "\n",
    "def get_conv_filter(data_dict, name):\n",
    "    return tf.constant(data_dict[name][0], name=\"filter\")\n",
    "\n",
    "def get_bias(data_dict, name):\n",
    "    return tf.constant(data_dict[name][1], name=\"biases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "\n",
    "# feel free to try different layers\n",
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "def stylize(content, styles, network_path='libs/imagenet-vgg-verydeep-19.mat', \n",
    "            iterations=1000, content_weight=5e0, content_weight_blend=0.5, style_weight=5e2, \n",
    "            style_layer_weight_exp=1,style_blend_weights=None, tv_weight=100,\n",
    "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "            print_iterations=100, checkpoint_iterations=100, checkpoint_path=None,\n",
    "            output_path=None):\n",
    "\n",
    "\n",
    "    shape = (1,) + content.shape                             #content image shape : (1,433,770,3)\n",
    "    style_shapes = [(1,) + style.shape for style in styles]  #style image shape : (1,600,800,3)\n",
    "    content_features = {}\n",
    "    style_features = [{} for _ in styles]\n",
    "\n",
    "    \n",
    "    # scale the importance of each style layers according to their depth. \n",
    "    # (deeper layers are more important if style_layers_weights > 1 (default = 1))\n",
    "    layer_weight = 1.0\n",
    "    style_layers_weights = {}                                # weight for different network layers\n",
    "    for style_layer in STYLE_LAYERS:                                    \n",
    "        style_layers_weights[style_layer] = layer_weight       #'relu1_1','relu2_1',...,'relu5_1'\n",
    "        layer_weight *= style_layer_weight_exp                 # 1.0\n",
    "\n",
    "\n",
    "    # normalize style layer weights\n",
    "    layer_weights_sum = 0\n",
    "    for style_layer in STYLE_LAYERS:                           #'relu1_1',..., 'relu5_1'\n",
    "        layer_weights_sum += style_layers_weights[style_layer] # 5.0\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] /= layer_weights_sum\n",
    "\n",
    "        \n",
    "    # FEATURE MAPS FROM CONTENT IMAGE\n",
    "    # compute the feature map of the content image by feeding it into the network\n",
    "    # the output net contains the features of each content layer\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=shape)\n",
    "\n",
    "        net = net_preloaded(image, pooling)             # {'conv1_1':Tensor,relu1_1:Tensor...}\n",
    "        content_pre = np.array([preprocess(content)])   # (1,433,770,3) subtract the mean pixel\n",
    "        for layer in CONTENT_LAYERS:                    #'relu4_2', 'relu5_2'\n",
    "            content_features[layer] = net[layer].eval(feed_dict={image: content_pre})  \n",
    "\n",
    "            \n",
    "    # FEATURE MAPS (GRAM MATRICES) FROM STYLE IMAGE\n",
    "    # compute style features of the style image by feeding it into the network\n",
    "    # and calculate the gram matrix\n",
    "    for i in range(len(styles)):\n",
    "        g = tf.Graph()\n",
    "        with g.as_default(), tf.Session() as sess:\n",
    "            image = tf.placeholder('float', shape=style_shapes[i])\n",
    "            net = net_preloaded(image, pooling)                           \n",
    "            style_pre = np.array([preprocess(styles[i])])\n",
    "            for layer in STYLE_LAYERS:              #'relu1_1', 'relu2_1',..., 'relu5_1'\n",
    "                features = net[layer].eval(feed_dict={image: style_pre})  # relu_1:(1,600,800,64)\n",
    "                features = np.reshape(features, (-1, features.shape[3]))  # (480000, 64)\n",
    "                gram = np.matmul(features.T, features) / features.size    # (64,64)\n",
    "                style_features[i][layer] = gram\n",
    "\n",
    "                \n",
    "    # make stylized image using backpropogation\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        # Generate a random image (the output image) with the same shape as the content image\n",
    "        initial = tf.random_normal(shape) * 0.256  \n",
    "        image = tf.Variable(initial)\n",
    "        net = net_preloaded(image, pooling)\n",
    "\n",
    "\n",
    "        # CONTENT LOSS\n",
    "        # we can adjust the weight of each content layers\n",
    "        # content_weight_blend is the ratio of two used content layers in this example\n",
    "        content_layers_weights = {}\n",
    "        content_layers_weights['relu4_2'] = content_weight_blend \n",
    "        content_layers_weights['relu5_2'] = 1.0 - content_weight_blend      \n",
    "\n",
    "        content_loss = 0\n",
    "        content_losses = []\n",
    "        for content_layer in CONTENT_LAYERS:\n",
    "            # Use MSE as content losses\n",
    "            # content weight is the coefficient for content loss\n",
    "            content_losses.append(content_layers_weights[content_layer] * content_weight * \n",
    "                                  (2 * tf.nn.l2_loss(net[content_layer] - content_features[content_layer]) /\n",
    "                                   content_features[content_layer].size))\n",
    "        content_loss += reduce(tf.add, content_losses)\n",
    "\n",
    "\n",
    "\n",
    "        # STYLE LOSS\n",
    "        # We can specify different weight for different style images\n",
    "        # style_layers_weights => weight for different network layers\n",
    "        # style_blend_weights => weight between different style images\n",
    "\n",
    "        if style_blend_weights is None:\n",
    "            style_blend_weights = [1.0/len(style_images) for _ in style_images]\n",
    "        else:\n",
    "            total_blend_weight = sum(style_blend_weights)\n",
    "            # normalization\n",
    "            style_blend_weights = [weight/total_blend_weight\n",
    "                                   for weight in style_blend_weights]\n",
    "\n",
    "\n",
    "        style_loss = 0\n",
    "        # iterate to calculate style loss with multiple style images\n",
    "        for i in range(len(styles)):\n",
    "            style_losses = []\n",
    "            for style_layer in STYLE_LAYERS:             # e.g. relu1_1\n",
    "                layer = net[style_layer]                   # relu1_1 of output image:(1,433,770,64)\n",
    "                _, height, width, number = map(lambda i: i.value, layer.get_shape())  \n",
    "                size = height * width * number\n",
    "                feats = tf.reshape(layer, (-1, number))    # (333410,64)\n",
    "\n",
    "                # Gram matrix for the features in relu1_1 of the output image.\n",
    "                gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "                # Gram matrix for the features in relu1_1 of the style image\n",
    "                style_gram = style_features[i][style_layer]   \n",
    "\n",
    "                # Style loss is the MSE for the difference of the 2 Gram matrices\n",
    "                style_losses.append(style_layers_weights[style_layer] * 2 * \n",
    "                                    tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "\n",
    "\n",
    "        # TOTAL VARIATION LOSS  \n",
    "        # Total variation denoising to do smoothing; \n",
    "        # cost to penalize neighboring pixel not used by the original paper by Gatys et al\n",
    "        # According to the paper Mahendran, Aravindh, and Andrea Vedaldi. \n",
    "        # \"Understanding deep image representations by inverting them.\"\n",
    "        # Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\n",
    "        tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "        tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "        tv_loss = tv_weight * 2 * (\n",
    "            (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
    "             tv_y_size) +\n",
    "            (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
    "             tv_x_size))\n",
    "\n",
    "\n",
    "        #OVERALL LOSS\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
    "\n",
    "        def print_progress():\n",
    "            print('     iteration: %d\\n' % i)\n",
    "            print('  content loss: %g\\n' % content_loss.eval())\n",
    "            print('    style loss: %g\\n' % style_loss.eval())\n",
    "            print('       tv loss: %g\\n' % tv_loss.eval())\n",
    "            print('    total loss: %g\\n' % loss.eval())\n",
    "\n",
    "        def imsave(path, img):\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "            Image.fromarray(img).save(path, quality=95)\n",
    "\n",
    "    \n",
    "    \n",
    "        # TRAINING\n",
    "        best_loss = float('inf')\n",
    "        best = None\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            if (print_iterations and print_iterations != 0):\n",
    "                print_progress()\n",
    "\n",
    "            for i in range(iterations):\n",
    "\n",
    "                train_step.run()\n",
    "\n",
    "                last_step = (i == iterations - 1)\n",
    "                if last_step or (print_iterations and i % print_iterations == 0):\n",
    "                    print_progress()\n",
    "\n",
    "                # store output and checkpoint images\n",
    "                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
    "                    this_loss = loss.eval()\n",
    "                    if this_loss < best_loss:\n",
    "                        best_loss = this_loss\n",
    "                        best = image.eval()\n",
    "\n",
    "                    img_out = unprocess(best.reshape(shape[1:]))\n",
    "\n",
    "                    output_file = None\n",
    "                    if not last_step:\n",
    "                        if checkpoint_path:\n",
    "                            output_file = checkpoint_path % i\n",
    "                        else:\n",
    "                            output_file = output_path\n",
    "\n",
    "                        if output_file:\n",
    "                            imsave(output_file, img_out)\n",
    "\n",
    "    print(\"finish stylizing.\")\n",
    "\n",
    "\n",
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iteration: 0\n",
      "\n",
      "  content loss: 23257.1\n",
      "\n",
      "    style loss: 4.20293e+06\n",
      "\n",
      "       tv loss: 26.2109\n",
      "\n",
      "    total loss: 4.22621e+06\n",
      "\n",
      "     iteration: 0\n",
      "\n",
      "  content loss: 22203.9\n",
      "\n",
      "    style loss: 3.28381e+06\n",
      "\n",
      "       tv loss: 14786.2\n",
      "\n",
      "    total loss: 3.3208e+06\n",
      "\n",
      "     iteration: 100\n",
      "\n",
      "  content loss: 16238.1\n",
      "\n",
      "    style loss: 15822.8\n",
      "\n",
      "       tv loss: 35361.7\n",
      "\n",
      "    total loss: 67422.6\n",
      "\n",
      "     iteration: 200\n",
      "\n",
      "  content loss: 11199.5\n",
      "\n",
      "    style loss: 8593.78\n",
      "\n",
      "       tv loss: 32208.9\n",
      "\n",
      "    total loss: 52002.2\n",
      "\n",
      "     iteration: 300\n",
      "\n",
      "  content loss: 9152.35\n",
      "\n",
      "    style loss: 5675.38\n",
      "\n",
      "       tv loss: 31302.8\n",
      "\n",
      "    total loss: 46130.5\n",
      "\n",
      "     iteration: 400\n",
      "\n",
      "  content loss: 8032.57\n",
      "\n",
      "    style loss: 4742.38\n",
      "\n",
      "       tv loss: 30772.4\n",
      "\n",
      "    total loss: 43547.3\n",
      "\n",
      "     iteration: 500\n",
      "\n",
      "  content loss: 7363.53\n",
      "\n",
      "    style loss: 4341.7\n",
      "\n",
      "       tv loss: 30453.1\n",
      "\n",
      "    total loss: 42158.4\n",
      "\n",
      "     iteration: 600\n",
      "\n",
      "  content loss: 6935.17\n",
      "\n",
      "    style loss: 4121.37\n",
      "\n",
      "       tv loss: 30198.7\n",
      "\n",
      "    total loss: 41255.3\n",
      "\n",
      "     iteration: 700\n",
      "\n",
      "  content loss: 6635.89\n",
      "\n",
      "    style loss: 3996.56\n",
      "\n",
      "       tv loss: 30006.7\n",
      "\n",
      "    total loss: 40639.2\n",
      "\n",
      "     iteration: 800\n",
      "\n",
      "  content loss: 6395.76\n",
      "\n",
      "    style loss: 3929.57\n",
      "\n",
      "       tv loss: 29876.8\n",
      "\n",
      "    total loss: 40202.1\n",
      "\n",
      "     iteration: 900\n",
      "\n",
      "  content loss: 6197.17\n",
      "\n",
      "    style loss: 4120.76\n",
      "\n",
      "       tv loss: 29648.4\n",
      "\n",
      "    total loss: 39966.3\n",
      "\n",
      "     iteration: 999\n",
      "\n",
      "  content loss: 6074.72\n",
      "\n",
      "    style loss: 4059.43\n",
      "\n",
      "       tv loss: 30024.7\n",
      "\n",
      "    total loss: 40158.9\n",
      "\n",
      "finish stylizing.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path=None\n",
    "output_path='image_output/wonder-woman-starry-night-tvweight-vgg16-100.jpg'\n",
    "\n",
    "stylize(content_image, style_images, iterations=1000,\n",
    "        content_weight=5e0, content_weight_blend=1, style_weight=5e2, \n",
    "        style_layer_weight_exp=1, style_blend_weights=None, tv_weight=100,\n",
    "        learning_rate=1e1, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "        print_iterations=100, checkpoint_iterations=100, checkpoint_path=checkpoint_path,\n",
    "        output_path=output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"16\"和\"19\"表示網絡中的需要更新需要weight（要學習的參數）的網絡層數，16的結果比較偏向風格，19就像助教的結果一樣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try changing the weights for the style, content, and denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "VGG19_LAYERS = (\n",
    "    'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "    'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "\n",
    "    'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "    'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "    'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "    'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "    'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "    'relu5_3', 'conv5_4', 'relu5_4'\n",
    ")\n",
    "\n",
    "\n",
    "def net_preloaded(input_image, pooling):\n",
    "    data_dict = np.load('libs/vgg19.npy', encoding='latin1').item()\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(VGG19_LAYERS):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels = get_conv_filter(data_dict, name)\n",
    "            # kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "\n",
    "            bias = get_bias(data_dict, name)\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "\n",
    "            # bias = bias.reshape(-1)\n",
    "            current = conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = pool_layer(current, pooling)\n",
    "\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(VGG19_LAYERS)\n",
    "    return net\n",
    "\n",
    "def conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, weights, strides=(1, 1, 1, 1), padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def pool_layer(input, pooling):\n",
    "    if pooling == 'avg':\n",
    "        return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                              padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                              padding='SAME')\n",
    "\n",
    "# before we feed the image into the network, we preprocess it by \n",
    "# extracting the mean_pixel from it.\n",
    "def preprocess(image):\n",
    "    return image - VGG_MEAN\n",
    "\n",
    "# remember to unprocess it before you plot it out and save it.\n",
    "def unprocess(image):\n",
    "    return image + VGG_MEAN\n",
    "\n",
    "def get_conv_filter(data_dict, name):\n",
    "    return tf.constant(data_dict[name][0], name=\"filter\")\n",
    "\n",
    "def get_bias(data_dict, name):\n",
    "    return tf.constant(data_dict[name][1], name=\"biases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "\n",
    "# feel free to try different layers\n",
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "def stylize(content, styles, network_path='libs/imagenet-vgg-verydeep-19.mat', \n",
    "            iterations=1000, content_weight=5e0, content_weight_blend=0.5, style_weight=5e2, \n",
    "            style_layer_weight_exp=1,style_blend_weights=None, tv_weight=100,\n",
    "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "            print_iterations=100, checkpoint_iterations=100, checkpoint_path=None,\n",
    "            output_path=None):\n",
    "\n",
    "\n",
    "    shape = (1,) + content.shape                             #content image shape : (1,433,770,3)\n",
    "    style_shapes = [(1,) + style.shape for style in styles]  #style image shape : (1,600,800,3)\n",
    "    content_features = {}\n",
    "    style_features = [{} for _ in styles]\n",
    "\n",
    "    \n",
    "    # scale the importance of each style layers according to their depth. \n",
    "    # (deeper layers are more important if style_layers_weights > 1 (default = 1))\n",
    "    layer_weight = 1.0\n",
    "    style_layers_weights = {}                                # weight for different network layers\n",
    "    for style_layer in STYLE_LAYERS:                                    \n",
    "        style_layers_weights[style_layer] = layer_weight       #'relu1_1','relu2_1',...,'relu5_1'\n",
    "        layer_weight *= style_layer_weight_exp                 # 1.0\n",
    "\n",
    "\n",
    "    # normalize style layer weights\n",
    "    layer_weights_sum = 0\n",
    "    for style_layer in STYLE_LAYERS:                         #'relu1_1',..., 'relu5_1'\n",
    "        layer_weights_sum += style_layers_weights[style_layer] # 5.0\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] /= layer_weights_sum\n",
    "\n",
    "        \n",
    "    # FEATURE MAPS FROM CONTENT IMAGE\n",
    "    # compute the feature map of the content image by feeding it into the network\n",
    "    # the output net contains the features of each content layer\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=shape)\n",
    "\n",
    "        net = net_preloaded(image, pooling)             # {'conv1_1':Tensor,relu1_1:Tensor...}\n",
    "        content_pre = np.array([preprocess(content)])   # (1,433,770,3) subtract the mean pixel\n",
    "        for layer in CONTENT_LAYERS:                    #'relu4_2', 'relu5_2'\n",
    "            content_features[layer] = net[layer].eval(feed_dict={image: content_pre})  \n",
    "\n",
    "            \n",
    "    # FEATURE MAPS (GRAM MATRICES) FROM STYLE IMAGE\n",
    "    # compute style features of the style image by feeding it into the network\n",
    "    # and calculate the gram matrix\n",
    "    for i in range(len(styles)):\n",
    "        g = tf.Graph()\n",
    "        with g.as_default(), tf.Session() as sess:\n",
    "            image = tf.placeholder('float', shape=style_shapes[i])\n",
    "            net = net_preloaded(image, pooling)                           \n",
    "            style_pre = np.array([preprocess(styles[i])])\n",
    "            for layer in STYLE_LAYERS:              #'relu1_1', 'relu2_1',..., 'relu5_1'\n",
    "                features = net[layer].eval(feed_dict={image: style_pre})  # relu_1:(1,600,800,64)\n",
    "                features = np.reshape(features, (-1, features.shape[3]))  # (480000, 64)\n",
    "                gram = np.matmul(features.T, features) / features.size    # (64,64)\n",
    "                style_features[i][layer] = gram\n",
    "\n",
    "                \n",
    "    # make stylized image using backpropogation\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        # Generate a random image (the output image) with the same shape as the content image\n",
    "        initial = tf.random_normal(shape) * 0.256  \n",
    "        image = tf.Variable(initial)\n",
    "        net = net_preloaded(image, pooling)\n",
    "\n",
    "\n",
    "        # CONTENT LOSS\n",
    "        # we can adjust the weight of each content layers\n",
    "        # content_weight_blend is the ratio of two used content layers in this example\n",
    "        content_layers_weights = {}\n",
    "        content_layers_weights['relu4_2'] = content_weight_blend \n",
    "        content_layers_weights['relu5_2'] = 1.0 - content_weight_blend      \n",
    "\n",
    "        content_loss = 0\n",
    "        content_losses = []\n",
    "        for content_layer in CONTENT_LAYERS:\n",
    "            # Use MSE as content losses\n",
    "            # content weight is the coefficient for content loss\n",
    "            content_losses.append(content_layers_weights[content_layer] * content_weight * \n",
    "                                  (2 * tf.nn.l2_loss(net[content_layer] - content_features[content_layer]) /\n",
    "                                   content_features[content_layer].size))\n",
    "        content_loss += reduce(tf.add, content_losses)\n",
    "\n",
    "\n",
    "\n",
    "        # STYLE LOSS\n",
    "        # We can specify different weight for different style images\n",
    "        # style_layers_weights => weight for different network layers\n",
    "        # style_blend_weights => weight between different style images\n",
    "\n",
    "        if style_blend_weights is None:\n",
    "            style_blend_weights = [1.0/len(style_images) for _ in style_images]\n",
    "        else:\n",
    "            total_blend_weight = sum(style_blend_weights)\n",
    "            # normalization\n",
    "            style_blend_weights = [weight/total_blend_weight\n",
    "                                   for weight in style_blend_weights]\n",
    "\n",
    "\n",
    "        style_loss = 0\n",
    "        # iterate to calculate style loss with multiple style images\n",
    "        for i in range(len(styles)):\n",
    "            style_losses = []\n",
    "            for style_layer in STYLE_LAYERS:             # e.g. relu1_1\n",
    "                layer = net[style_layer]                   # relu1_1 of output image:(1,433,770,64)\n",
    "                _, height, width, number = map(lambda i: i.value, layer.get_shape())  \n",
    "                size = height * width * number\n",
    "                feats = tf.reshape(layer, (-1, number))    # (333410,64)\n",
    "\n",
    "                # Gram matrix for the features in relu1_1 of the output image.\n",
    "                gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "                # Gram matrix for the features in relu1_1 of the style image\n",
    "                style_gram = style_features[i][style_layer]   \n",
    "\n",
    "                # Style loss is the MSE for the difference of the 2 Gram matrices\n",
    "                style_losses.append(style_layers_weights[style_layer] * 2 * \n",
    "                                    tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "\n",
    "\n",
    "        # TOTAL VARIATION LOSS  \n",
    "        # Total variation denoising to do smoothing; \n",
    "        # cost to penalize neighboring pixel not used by the original paper by Gatys et al\n",
    "        # According to the paper Mahendran, Aravindh, and Andrea Vedaldi. \n",
    "        # \"Understanding deep image representations by inverting them.\"\n",
    "        # Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\n",
    "        tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "        tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "        tv_loss = tv_weight * 2 * (\n",
    "            (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
    "             tv_y_size) +\n",
    "            (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
    "             tv_x_size))\n",
    "\n",
    "\n",
    "        #OVERALL LOSS\n",
    "        loss = content_loss + 2*style_loss + 2/3*tv_loss\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
    "\n",
    "        def print_progress():\n",
    "            print('     iteration: %d\\n' % i)\n",
    "            print('  content loss: %g\\n' % content_loss.eval())\n",
    "            print('    style loss: %g\\n' % style_loss.eval())\n",
    "            print('       tv loss: %g\\n' % tv_loss.eval())\n",
    "            print('    total loss: %g\\n' % loss.eval())\n",
    "\n",
    "        def imsave(path, img):\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "            Image.fromarray(img).save(path, quality=95)\n",
    "\n",
    "    \n",
    "    \n",
    "        # TRAINING\n",
    "        best_loss = float('inf')\n",
    "        best = None\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            if (print_iterations and print_iterations != 0):\n",
    "                print_progress()\n",
    "\n",
    "            for i in range(iterations):\n",
    "\n",
    "                train_step.run()\n",
    "\n",
    "                last_step = (i == iterations - 1)\n",
    "                if last_step or (print_iterations and i % print_iterations == 0):\n",
    "                    print_progress()\n",
    "\n",
    "                # store output and checkpoint images\n",
    "                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
    "                    this_loss = loss.eval()\n",
    "                    if this_loss < best_loss:\n",
    "                        best_loss = this_loss\n",
    "                        best = image.eval()\n",
    "\n",
    "                    img_out = unprocess(best.reshape(shape[1:]))\n",
    "\n",
    "                    output_file = None\n",
    "                    if not last_step:\n",
    "                        if checkpoint_path:\n",
    "                            output_file = checkpoint_path % i\n",
    "                        else:\n",
    "                            output_file = output_path\n",
    "\n",
    "                        if output_file:\n",
    "                            imsave(output_file, img_out)\n",
    "\n",
    "    print(\"finish stylizing.\")\n",
    "\n",
    "\n",
    "\n",
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iteration: 0\n",
      "\n",
      "  content loss: 383718\n",
      "\n",
      "    style loss: 1.5164e+06\n",
      "\n",
      "       tv loss: 26.2367\n",
      "\n",
      "    total loss: 3.41653e+06\n",
      "\n",
      "     iteration: 0\n",
      "\n",
      "  content loss: 367506\n",
      "\n",
      "    style loss: 1.26764e+06\n",
      "\n",
      "       tv loss: 15387.4\n",
      "\n",
      "    total loss: 2.91304e+06\n",
      "\n",
      "     iteration: 100\n",
      "\n",
      "  content loss: 109207\n",
      "\n",
      "    style loss: 14711.8\n",
      "\n",
      "       tv loss: 40723.6\n",
      "\n",
      "    total loss: 165780\n",
      "\n",
      "     iteration: 200\n",
      "\n",
      "  content loss: 95541.3\n",
      "\n",
      "    style loss: 11977.8\n",
      "\n",
      "       tv loss: 36128.6\n",
      "\n",
      "    total loss: 143583\n",
      "\n",
      "     iteration: 300\n",
      "\n",
      "  content loss: 90904.3\n",
      "\n",
      "    style loss: 11242.2\n",
      "\n",
      "       tv loss: 34707.7\n",
      "\n",
      "    total loss: 136527\n",
      "\n",
      "     iteration: 400\n",
      "\n",
      "  content loss: 91650\n",
      "\n",
      "    style loss: 10925.1\n",
      "\n",
      "       tv loss: 34260.5\n",
      "\n",
      "    total loss: 136340\n",
      "\n",
      "     iteration: 500\n",
      "\n",
      "  content loss: 87466.3\n",
      "\n",
      "    style loss: 11761.3\n",
      "\n",
      "       tv loss: 33864.5\n",
      "\n",
      "    total loss: 133565\n",
      "\n",
      "     iteration: 600\n",
      "\n",
      "  content loss: 86774.8\n",
      "\n",
      "    style loss: 13513.4\n",
      "\n",
      "       tv loss: 33951.2\n",
      "\n",
      "    total loss: 136436\n",
      "\n",
      "     iteration: 700\n",
      "\n",
      "  content loss: 90949.8\n",
      "\n",
      "    style loss: 10587.8\n",
      "\n",
      "       tv loss: 34022\n",
      "\n",
      "    total loss: 134807\n",
      "\n",
      "     iteration: 800\n",
      "\n",
      "  content loss: 96381.6\n",
      "\n",
      "    style loss: 12667.7\n",
      "\n",
      "       tv loss: 37565.2\n",
      "\n",
      "    total loss: 146760\n",
      "\n",
      "     iteration: 900\n",
      "\n",
      "  content loss: 87664.9\n",
      "\n",
      "    style loss: 12002.1\n",
      "\n",
      "       tv loss: 33619.5\n",
      "\n",
      "    total loss: 134082\n",
      "\n",
      "     iteration: 999\n",
      "\n",
      "  content loss: 91500.6\n",
      "\n",
      "    style loss: 10742.2\n",
      "\n",
      "       tv loss: 34229.3\n",
      "\n",
      "    total loss: 135805\n",
      "\n",
      "finish stylizing.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path=None\n",
    "output_path='image_output/wonder-woman-starry-night-tvweight-scd.jpg'\n",
    "\n",
    "stylize(content_image, style_images, iterations=1000,\n",
    "        content_weight=5e0, content_weight_blend=1, style_weight=5e2, \n",
    "        style_layer_weight_exp=1, style_blend_weights=None, tv_weight=100,\n",
    "        learning_rate=1e1, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "        print_iterations=100, checkpoint_iterations=100, checkpoint_path=checkpoint_path,\n",
    "        output_path=output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 因為將style的loss比例調中了，所以在做訓練的實踏會比較在意風格的更新，做出來的結果風格味道也較重一點。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use different weights for the content and style layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iteration: 0\n",
      "\n",
      "  content loss: 384249\n",
      "\n",
      "    style loss: 688916\n",
      "\n",
      "       tv loss: 26.1397\n",
      "\n",
      "    total loss: 1.7621e+06\n",
      "\n",
      "     iteration: 0\n",
      "\n",
      "  content loss: 368217\n",
      "\n",
      "    style loss: 599479\n",
      "\n",
      "       tv loss: 15528.1\n",
      "\n",
      "    total loss: 1.57753e+06\n",
      "\n",
      "     iteration: 100\n",
      "\n",
      "  content loss: 96750.6\n",
      "\n",
      "    style loss: 12589.9\n",
      "\n",
      "       tv loss: 30436.2\n",
      "\n",
      "    total loss: 142221\n",
      "\n",
      "     iteration: 200\n",
      "\n",
      "  content loss: 90273\n",
      "\n",
      "    style loss: 12190.6\n",
      "\n",
      "       tv loss: 26765.4\n",
      "\n",
      "    total loss: 132498\n",
      "\n",
      "     iteration: 300\n",
      "\n",
      "  content loss: 89427.6\n",
      "\n",
      "    style loss: 11377.8\n",
      "\n",
      "       tv loss: 24585.8\n",
      "\n",
      "    total loss: 128574\n",
      "\n",
      "     iteration: 400\n",
      "\n",
      "  content loss: 91938.1\n",
      "\n",
      "    style loss: 11108\n",
      "\n",
      "       tv loss: 22975.5\n",
      "\n",
      "    total loss: 129471\n",
      "\n",
      "     iteration: 500\n",
      "\n",
      "  content loss: 84396.7\n",
      "\n",
      "    style loss: 15610.6\n",
      "\n",
      "       tv loss: 23008.6\n",
      "\n",
      "    total loss: 130957\n",
      "\n",
      "     iteration: 600\n",
      "\n",
      "  content loss: 85044.6\n",
      "\n",
      "    style loss: 13770.3\n",
      "\n",
      "       tv loss: 22683.4\n",
      "\n",
      "    total loss: 127707\n",
      "\n",
      "     iteration: 700\n",
      "\n",
      "  content loss: 88313.9\n",
      "\n",
      "    style loss: 10626.6\n",
      "\n",
      "       tv loss: 22122.1\n",
      "\n",
      "    total loss: 124315\n",
      "\n",
      "     iteration: 800\n",
      "\n",
      "  content loss: 84118.4\n",
      "\n",
      "    style loss: 13334.1\n",
      "\n",
      "       tv loss: 23409.9\n",
      "\n",
      "    total loss: 126393\n",
      "\n",
      "     iteration: 900\n",
      "\n",
      "  content loss: 82923.8\n",
      "\n",
      "    style loss: 14166\n",
      "\n",
      "       tv loss: 23244.4\n",
      "\n",
      "    total loss: 126752\n",
      "\n",
      "     iteration: 999\n",
      "\n",
      "  content loss: 89746\n",
      "\n",
      "    style loss: 10175.6\n",
      "\n",
      "       tv loss: 21764\n",
      "\n",
      "    total loss: 124606\n",
      "\n",
      "finish stylizing.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path=None\n",
    "output_path='image_output/wonder-woman-starry-night-tvweight-diff_layers.jpg'\n",
    "\n",
    "stylize(content_image, style_images, iterations=1000,\n",
    "        content_weight=10e0, content_weight_blend=0.5, style_weight=10e2, \n",
    "        style_layer_weight_exp=10, style_blend_weights=None, tv_weight=100,\n",
    "        learning_rate=1e1, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "        print_iterations=100, checkpoint_iterations=100, checkpoint_path=checkpoint_path,\n",
    "        output_path=output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> 透過layer之間的權重調整，訓練出來的圖片顏色更鮮明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
