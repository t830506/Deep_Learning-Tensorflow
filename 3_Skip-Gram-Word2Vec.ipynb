{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model  \n",
    "We will now focus on building the model. Let's briefly go through what we will do next.  \n",
    "1. Define the inputs and outputs\n",
    "2. Define the weights\n",
    "3. Define the loss function\n",
    "4. Define the optimizer\n",
    "5. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unahsu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import # use absolute import instead of relative import\n",
    "\n",
    "# '/' for floating point division, '//' for integer division\n",
    "from __future__ import division  \n",
    "from __future__ import print_function  # use 'print' as a function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from process_data import make_dir, download, read_data, build_vocab, convert_words_to_index, process_data, generate_sample, get_batch, get_batch_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "    def __init__(self, hparams=None):\n",
    "\n",
    "        if hparams is None:\n",
    "            self.hps = get_default_hparams()\n",
    "        else:\n",
    "            self.hps = hparams\n",
    "\n",
    "        # define a variable to record training progress\n",
    "        self.global_step = tf.Variable(\n",
    "            0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def _create_input(self):\n",
    "        \"\"\" Step 1: define input and output \"\"\"\n",
    "\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.centers = tf.placeholder(tf.int32, [self.hps.num_pairs], name='centers')\n",
    "            self.targets = tf.placeholder(tf.int32, [self.hps.num_pairs, 1], name='targets')\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((self.centers, self.targets))\n",
    "            dataset = dataset.repeat() # Repeat the input indefinitely\n",
    "            dataset = dataset.batch(self.hps.batch_size)\n",
    "\n",
    "            self.iterator = dataset.make_initializable_iterator()  # create iterator\n",
    "            self.center_words, self.target_words = self.iterator.get_next()\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        \"\"\"\n",
    "        Step 2: define weights. \n",
    "        In word2vec, it's actually the weights that we care about\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"embed\"):\n",
    "                self.embed_matrix = tf.Variable(\n",
    "                    tf.random_uniform([self.hps.vocab_size, #10000X128\n",
    "                                       self.hps.embed_size], -1.0, 1.0),\n",
    "                    name='embed_matrix')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\" Step 3 + 4: define the model + the loss function \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # Step 3: define the inference\n",
    "                embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "\n",
    "                # Step 4: define loss function\n",
    "                # construct variables for NCE loss\n",
    "                nce_weight = tf.Variable(\n",
    "                    tf.truncated_normal([self.hps.vocab_size, self.hps.embed_size],\n",
    "                                        stddev=1.0 / (self.hps.embed_size ** 0.5)),\n",
    "                    name='nce_weight')\n",
    "                nce_bias = tf.Variable(tf.zeros([self.hps.vocab_size]), name='nce_bias')\n",
    "\n",
    "                # define loss function to be NCE loss function\n",
    "                self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                          biases=nce_bias,\n",
    "                                                          labels=self.target_words,\n",
    "                                                          inputs=embed,\n",
    "                                                          num_sampled=self.hps.num_sampled,\n",
    "                                                          num_classes=self.hps.vocab_size),\n",
    "                                           name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.hps.lr).minimize(\n",
    "                self.loss, global_step=self.global_step)\n",
    "\n",
    "    def _build_nearby_graph(self):\n",
    "        # Nodes for computing neighbors for a given word according to their cosine distance.\n",
    "        self.nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n",
    "        \n",
    "        nemb = tf.nn.l2_normalize(self.embed_matrix, 1)\n",
    "        nearby_emb = tf.gather(nemb, self.nearby_word)\n",
    "        nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n",
    "        self.nearby_val, self.nearby_idx = tf.nn.top_k(nearby_dist, min(1000, self.hps.vocab_size))\n",
    "\n",
    "    def _build_eval_graph(self):\n",
    "        \"\"\"Build the eval graph.\n",
    "        Eval graph\n",
    "\n",
    "        Each analogy task is to predict the 4th word (d) given three words: a, b, c.  E.g., a=italy, b=rome, c=france,\n",
    "        we should predict d=paris.\n",
    "\n",
    "        The eval feeds three vectors of word ids for a, b, c, \n",
    "        each of which is of size N, where N is the number of analogies we want to evaluate in one batch.\n",
    "        \"\"\"\n",
    "        self.analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "        self.analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "        self.analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "\n",
    "        # Normalized word embeddings of shape [vocab_size, emb_dim].\n",
    "        nemb = tf.nn.l2_normalize(self.embed_matrix, 1)\n",
    "\n",
    "        # Each row of a_emb, b_emb, c_emb is a word's embedding vector.\n",
    "        # They all have the shape [N, emb_dim]\n",
    "        a_emb = tf.gather(nemb, self.analogy_a)  # a's embs\n",
    "        b_emb = tf.gather(nemb, self.analogy_b)  # b's embs\n",
    "        c_emb = tf.gather(nemb, self.analogy_c)  # c's embs\n",
    "\n",
    "        # We expect that d's embedding vectors on the unit hyper-sphere is near:\n",
    "        # c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n",
    "        target = c_emb + (b_emb - a_emb)\n",
    "\n",
    "        # Compute cosine distance between each pair of target and vocab.\n",
    "        # dist has shape [N, vocab_size].\n",
    "        dist = tf.matmul(target, nemb, transpose_b=True)\n",
    "\n",
    "        # For each question (row in dist), find the top 20 words.\n",
    "        _, self.pred_idx = tf.nn.top_k(dist, 20)\n",
    "\n",
    "    def predict(self, sess, analogy):\n",
    "        \"\"\" Predict the top 20 answers for analogy questions \"\"\"\n",
    "        idx, = sess.run([self.pred_idx], {\n",
    "            self.analogy_a: analogy[:, 0],\n",
    "            self.analogy_b: analogy[:, 1],\n",
    "            self.analogy_c: analogy[:, 2]\n",
    "        })\n",
    "        return idx\n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_input()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._build_eval_graph()\n",
    "        self._build_nearby_graph()\n",
    "        self._create_summaries()\n",
    "\n",
    "def train_model(sess, model, batch_gen, index_words, num_train_steps):\n",
    "    saver = tf.train.Saver()\n",
    "    # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    initial_step = 0\n",
    "    make_dir('checkpoints') # directory to store checkpoints\n",
    "\n",
    "    sess.run(tf.global_variables_initializer()) # initialize all variables\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    total_loss = 0.0 # use this to calculate late average loss in the last SKIP_STEP steps\n",
    "    writer = tf.summary.FileWriter('graph/lr' + str(model.hps.lr), sess.graph)\n",
    "    initial_step = model.global_step.eval()\n",
    "    for index in range(initial_step, initial_step + num_train_steps):\n",
    "        # feed in new dataset  \n",
    "        if index % model.hps.new_dataset_every == 0:\n",
    "            try:\n",
    "                centers, targets = next(batch_gen)\n",
    "            except StopIteration: # generator has nothing left to generate\n",
    "                batch_gen = get_batch_gen(index_words, \n",
    "                                          model.hps.skip_window, \n",
    "                                          model.hps.num_pairs)\n",
    "                centers, targets = next(batch_gen)\n",
    "                print('Finished looking at the whole text')\n",
    "            \n",
    "            feed = {\n",
    "                model.centers: centers,\n",
    "                model.targets: targets\n",
    "            }\n",
    "            _ = sess.run(model.iterator.initializer, feed_dict = feed)\n",
    "            print('feeding in new dataset')\n",
    "\n",
    "    loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op])\n",
    "    writer.add_summary(summary, global_step=index)\n",
    "    total_loss += loss_batch\n",
    "    if (index + 1) % model.hps.skip_step == 0:\n",
    "        print('Average loss at step {}: {:5.1f}'.format(\n",
    "                                                  index, total_loss/model.hps.skip_step))\n",
    "        total_loss = 0.0\n",
    "        saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "\n",
    "def get_default_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        num_pairs = 10**6,                # number of (center, target) pairs in each dataset instance\n",
    "        vocab_size = 10000,\n",
    "        batch_size = 128,\n",
    "        embed_size = 300,                 # dimension of the word embedding vectors\n",
    "        skip_window = 5,                  # the context window\n",
    "        num_sampled = 100,                # number of negative examples to sample\n",
    "        lr = 0.005,                       # learning rate\n",
    "        new_dataset_every = 10**4,        # replace the original dataset every ? steps\n",
    "        num_train_steps = 8*10**5,        # number of training steps for each feed of dataset\n",
    "        skip_step = 2000\n",
    "    )\n",
    "    return hparams\n",
    "\n",
    "def main():\n",
    "    hps = get_default_hparams()\n",
    "    index_words, dictionary, index_dictionary = process_data(hps.vocab_size)\n",
    "    batch_gen = get_batch_gen(index_words, hps.skip_window, hps.num_pairs)\n",
    "\n",
    "    model = SkipGramModel(hparams = hps)\n",
    "    model.build_graph()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # feed the model with dataset\n",
    "        centers, targets = next(batch_gen)\n",
    "        feed = {\n",
    "            model.centers: centers,\n",
    "            model.targets: targets\n",
    "        }\n",
    "        sess.run(model.iterator.initializer, feed_dict = feed) # initialize the iterator\n",
    "\n",
    "        train_model(sess, model, batch_gen, index_words, hps.num_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n",
      "feeding in new dataset\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from process_data import process_data\n",
    "from train import get_default_hparams, SkipGramModel\n",
    "\n",
    "#Clears the default graph stack and resets the global default graph\n",
    "tf.reset_default_graph() \n",
    "hps = get_default_hparams()\n",
    "\n",
    "# get dictionary \n",
    "index_words, dictionary, index_dictionary = process_data(hps.vocab_size)\n",
    "\n",
    "# build model\n",
    "model = SkipGramModel(hps)\n",
    "model.build_graph()\n",
    "\n",
    "# initialize variables and restore checkpoint\n",
    "config = tf.ConfigProto(allow_soft_placement = True, log_device_placement = True)\n",
    "#sess = tf.Session(config = config)\n",
    "sess = tf.InteractiveSession(config = config)\n",
    "init = tf.global_variables_initializer() # an operation that initializes all variables\n",
    "sess.run(init) # run the init operation with session\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nearby(words, model, sess, dictionary, index_dictionary, num=20):\n",
    "    \"\"\"Prints out nearby words given a list of words.\"\"\"\n",
    "    ids = np.array([dictionary.get(x, 0) for x in words])\n",
    "    vals, idx = sess.run(\n",
    "        [model.nearby_val, model.nearby_idx], {model.nearby_word: ids})\n",
    "    for i in range(len(words)):\n",
    "        print(\"\\n%s\\n=====================================\" % (words[i]))\n",
    "        for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "            print(\"%-20s %6.4f\" % (index_dictionary.get(neighbor), distance))\n",
    "        \n",
    "def analogy(line, model, sess, dictionary, index_dictionary):\n",
    "    \"\"\" Prints the top k anologies for a given array which contain 3 words\"\"\"\n",
    "    analogy = np.array([dictionary.get(w, 0) for w in line])[np.newaxis,:]\n",
    "    idx = model.predict(sess, analogy)\n",
    "    print(line)\n",
    "    for i in idx[0]:\n",
    "        print(index_dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['machine', 'learning']\n",
    "nearby(words, model, sess, dictionary, index_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(['london', 'england', 'berlin'], model, sess, dictionary, index_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = 300\n",
    "\n",
    "embed_matrix = sess.run(model.embed_matrix) # get the embed matrix\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embed_matrix[:rng])\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "\n",
    "for i in range(rng):\n",
    "    plt.scatter(X_embedded[i][0], X_embedded[i][1])\n",
    "    plt.text(X_embedded[i][0]+0.2,\n",
    "             X_embedded[i][1]+0.2,\n",
    "             index_dictionary.get(i, 0), fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
