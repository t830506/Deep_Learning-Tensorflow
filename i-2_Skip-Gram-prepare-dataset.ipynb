{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram and CBOW  \n",
    "Word2Vec comes in two variants Skip-Gram and CBOW (Continuous Bag-Of-Words). Algorithmically, these models are similar.  \n",
    "CBOW predicts the target words using its neighborhood(context) whereas Skip-Gram does the inverse, which is to predict context words from the target words. For example, given the sentence the quick brown fox jumped over the lazy dog. Defining the context words as the word to the left and right of the target word, CBOW will be trained on the dataset:  \n",
    "([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox)...  \n",
    "where CBOW tries to predict the target word quick from the context words in brackets [the, brown], and predict brown from [quick, fox] and so on. However, with Skip-Gram, the dataset becomes  \n",
    "(quick, the), (quick, brown), (brown, quick), (brown, fox), ...  \n",
    "where Skip-Gram predicts the context word the, brown with the target word quick. Statistically, CBOW smoothes over a lot of the distributional information (by treating an entire context as one example). For the most part, this turns out to be a useful thing for smaller datasets.  \n",
    "On the other hand, Skip-Gram treats each context-target pair as a new observation and is shown to be able to capture the semantics better when we have a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data  \n",
    "To generate batches for training, several functions defined below are used.  \n",
    "First, we read the data into the memory and build the vocabulary using a number of most commonly seen words.  \n",
    "Meanwhile, we build keep two dictionaries, a dictionary that translates words to indices and another which does the reverse.  \n",
    "Then, for every word in the text selected as the center word, pair them with one of the context words.  \n",
    "Finally, a python generator which generates a batch of pairs of center-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"The content of process_data.py\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_FOLDER = ''\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    file_path = DATA_FOLDER + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception(\n",
    "              'File ' + file_name +\n",
    "              ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return file_path    \n",
    "    \n",
    "    \n",
    "def read_data(file_path): #string(words)\n",
    "    \"\"\" Read data into a list of tokens\"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "        # tf.compat.as_str() converts the input into the string\n",
    "    return words\n",
    "\n",
    "def build_vocab(words, vocab_size): #two dictionaries(dictionary, index_dictionary)\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    index = 0\n",
    "    make_dir('processed')\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if index < 1000:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "\n",
    "def process_data(vocab_size): #string_to_number(index_words)\n",
    "    \"\"\" Read data, build vocabulary and dictionary\"\"\"\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    dictionary, index_dictionary = build_vocab(words, vocab_size)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words # to save memory\n",
    "    return index_words, dictionary, index_dictionary\n",
    "\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        context = random.randint(1, context_window_size)#1~5\n",
    "        # get a random target before the center word\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # get a random target after the center wrod\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        yield center_batch, target_batch\n",
    "\n",
    "def get_batch_gen(index_words, context_window_size, batch_size):\n",
    "    \"\"\" Return a python generator that generates batches\"\"\"\n",
    "    single_gen = generate_sample(index_words, context_window_size)\n",
    "    batch_gen = get_batch(single_gen, batch_size)\n",
    "    return batch_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "(64,)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "window_sz = 5\n",
    "batch_sz = 64\n",
    "\n",
    "index_words, dictionary, index_dictionary = process_data(vocab_size)\n",
    "batch_gen = get_batch_gen(index_words, window_sz, batch_sz)\n",
    "X, y = next(batch_gen) #X=center, y=neighbor(number of neighbor is random(1~5) )\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class UNK including the UNK of the english revolution and the UNK UNK of the french revolution whilst the term is still used in a UNK way to describe any act that used violent means to destroy the "
     ]
    }
   ],
   "source": [
    "for i in range(50): # print out the first 10 words in the text\n",
    "    print(index_dictionary[index_words[i]], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( anarchism originated )\n",
      "( anarchism as )\n",
      "( anarchism a )\n",
      "( anarchism term )\n",
      "( anarchism of )\n",
      "( originated anarchism )\n",
      "( originated as )\n",
      "( as anarchism )\n",
      "( as originated )\n",
      "( as a )\n",
      "( as term )\n",
      "( a originated )\n",
      "( a as )\n",
      "( a term )\n",
      "( a of )\n",
      "( term anarchism )\n",
      "( term originated )\n",
      "( term as )\n",
      "( term a )\n",
      "( term of )\n",
      "( term abuse )\n",
      "( term first )\n",
      "( term used )\n",
      "( term against )\n",
      "( of term )\n",
      "( of abuse )\n",
      "( abuse as )\n",
      "( abuse a )\n",
      "( abuse term )\n",
      "( abuse of )\n",
      "( abuse first )\n",
      "( abuse used )\n",
      "( abuse against )\n",
      "( abuse early )\n",
      "( first a )\n",
      "( first term )\n",
      "( first of )\n",
      "( first abuse )\n",
      "( first used )\n",
      "( first against )\n",
      "( first early )\n",
      "( first working )\n",
      "( used term )\n",
      "( used of )\n",
      "( used abuse )\n",
      "( used first )\n",
      "( used against )\n",
      "( used early )\n",
      "( used working )\n",
      "( used class )\n",
      "( against term )\n",
      "( against of )\n",
      "( against abuse )\n",
      "( against first )\n",
      "( against used )\n",
      "( against early )\n",
      "( against working )\n",
      "( against class )\n",
      "( against UNK )\n",
      "( against including )\n",
      "( early abuse )\n",
      "( early first )\n",
      "( early used )\n",
      "( early against )\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)): # print out the pairs\n",
    "    data = index_dictionary[X[i]]\n",
    "    label = index_dictionary[y[i,0]]\n",
    "    print('(', data, label,')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using the Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.repeat()          # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(BATCH_SIZE) # stack BATCH_SIZE elements into one\n",
    "iterator = dataset.make_one_shot_iterator() # iterator\n",
    "next_batch = iterator.get_next()    # an operation that gives the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    data, label = sess.run(next_batch)\n",
    "    print(data.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
