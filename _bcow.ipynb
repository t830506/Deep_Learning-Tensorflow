{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"The content of process_data.py\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "EXPECTED_BYTES = 31344016\n",
    "DATA_FOLDER = ''\n",
    "FILE_NAME = 'text8.zip'\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    file_path = DATA_FOLDER + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)\n",
    "    file_stat = os.stat(file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception(\n",
    "              'File ' + file_name +\n",
    "              ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return file_path    \n",
    "    \n",
    "    \n",
    "def read_data(file_path): #string(words)\n",
    "    \"\"\" Read data into a list of tokens\"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "        # tf.compat.as_str() converts the input into the string\n",
    "    return words\n",
    "\n",
    "def build_vocab(words, vocab_size): #two dictionaries(dictionary, index_dictionary)\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words \"\"\"\n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    index = 0\n",
    "    make_dir('processed')\n",
    "    with open('processed/vocab_1000.tsv', \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = index\n",
    "            if index < 1000:\n",
    "                f.write(word + \"\\n\")\n",
    "            index += 1\n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, index_dictionary\n",
    "\n",
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "\n",
    "def process_data(vocab_size): #string_to_number(index_words)\n",
    "    \"\"\" Read data, build vocabulary and dictionary\"\"\"\n",
    "    file_path = download(FILE_NAME, EXPECTED_BYTES)\n",
    "    words = read_data(file_path)\n",
    "    dictionary, index_dictionary = build_vocab(words, vocab_size)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words # to save memory\n",
    "    return index_words, dictionary, index_dictionary\n",
    "\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        left = np.array(index_words[max(0, index - context_window_size)])\n",
    "        right = np.array(index_words[index + context_window_size])\n",
    "        target = np.append(left, right)\n",
    "        yield target, center\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, window_sz*2])\n",
    "        for index in range(batch_size):\n",
    "            target_batch[index], center_batch[index] = next(iterator)\n",
    "        yield target_batch, center_batch\n",
    "\n",
    "def get_batch_gen(index_words, context_window_size, batch_size):\n",
    "    \"\"\" Return a python generator that generates batches\"\"\"\n",
    "    single_gen = generate_sample(index_words, context_window_size)\n",
    "    batch_gen = get_batch(single_gen, batch_size)\n",
    "    return batch_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n",
      "(64, 2)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "window_sz = 1\n",
    "batch_sz = 64\n",
    "\n",
    "index_words, dictionary, index_dictionary = process_data(vocab_size)\n",
    "batch_gen = get_batch_gen(index_words, window_sz, batch_sz)\n",
    "X, y = next(batch_gen)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( ('anarchism', 'originated') anarchism )\n",
      "( ('anarchism', 'as') originated )\n",
      "( ('originated', 'a') as )\n",
      "( ('as', 'term') a )\n",
      "( ('a', 'of') term )\n",
      "( ('term', 'abuse') of )\n",
      "( ('of', 'first') abuse )\n",
      "( ('abuse', 'used') first )\n",
      "( ('first', 'against') used )\n",
      "( ('used', 'early') against )\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): # print out the pairs\n",
    "    data = index_dictionary[X[i, 0]], index_dictionary[X[i, 1]]\n",
    "    label = index_dictionary[y[i]]\n",
    "    print('(', data, label,')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import # use absolute import instead of relative import\n",
    "\n",
    "# '/' for floating point division, '//' for integer division\n",
    "from __future__ import division  \n",
    "from __future__ import print_function  # use 'print' as a function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from BCOW import make_dir, download, read_data, build_vocab, convert_words_to_index, process_data, generate_sample, get_batch, get_batch_gen\n",
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "    def __init__(self, hparams=None):\n",
    "\n",
    "        if hparams is None:\n",
    "            self.hps = get_default_hparams()\n",
    "        else:\n",
    "            self.hps = hparams\n",
    "\n",
    "        # define a variable to record training progress\n",
    "        self.global_step = tf.Variable(\n",
    "            0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def _create_input(self):\n",
    "        \"\"\" Step 1: define input and output \"\"\"\n",
    "\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.centers = tf.placeholder(tf.int32, [self.hps.num_pairs, 2], name='centers')\n",
    "            self.targets = tf.placeholder(tf.int32, [self.hps.num_pairs], name='targets')\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((self.centers, self.targets))\n",
    "            dataset = dataset.repeat() # Repeat the input indefinitely\n",
    "            dataset = dataset.batch(self.hps.batch_size)\n",
    "\n",
    "            self.iterator = dataset.make_initializable_iterator()  # create iterator\n",
    "            self.center_words, self.target_words = self.iterator.get_next()\n",
    "\n",
    "    def _create_embedding(self):\n",
    "        \"\"\"\n",
    "        Step 2: define weights. \n",
    "        In word2vec, it's actually the weights that we care about\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"embed\"):\n",
    "                self.embed_matrix = tf.Variable(\n",
    "                    tf.random_uniform([self.hps.vocab_size, #10000X128\n",
    "                                       self.hps.embed_size*2], -1.0, 1.0),\n",
    "                    name='embed_matrix')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        \"\"\" Step 3 + 4: define the model + the loss function \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                # Step 3: define the inference\n",
    "                embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "\n",
    "                # Step 4: define loss function\n",
    "                # construct variables for NCE loss\n",
    "                nce_weight = tf.Variable(\n",
    "                    tf.truncated_normal([self.hps.vocab_size, self.hps.embed_size*2],\n",
    "                                        stddev=1.0 / (self.hps.embed_size ** 0.5)),\n",
    "                    name='nce_weight')\n",
    "                nce_bias = tf.Variable(tf.zeros([self.hps.vocab_size]), name='nce_bias')\n",
    "\n",
    "                # define loss function to be NCE loss function\n",
    "                self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                          biases=nce_bias,\n",
    "                                                          labels=self.target_words,\n",
    "                                                          inputs=embed,\n",
    "                                                          num_sampled=self.hps.num_sampled,\n",
    "                                                          num_classes=self.hps.vocab_size),\n",
    "                                           name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.hps.lr).minimize(\n",
    "                self.loss, global_step=self.global_step)\n",
    "\n",
    "    def _build_nearby_graph(self):\n",
    "        # Nodes for computing neighbors for a given word according to their cosine distance.\n",
    "        self.nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n",
    "        \n",
    "        nemb = tf.nn.l2_normalize(self.embed_matrix, 1)\n",
    "        nearby_emb = tf.gather(nemb, self.nearby_word)\n",
    "        nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n",
    "        self.nearby_val, self.nearby_idx = tf.nn.top_k(nearby_dist, min(1000, self.hps.vocab_size))\n",
    "\n",
    "    def _build_eval_graph(self):\n",
    "        \"\"\"Build the eval graph.\n",
    "        Eval graph\n",
    "\n",
    "        Each analogy task is to predict the 4th word (d) given three words: a, b, c.  E.g., a=italy, b=rome, c=france,\n",
    "        we should predict d=paris.\n",
    "\n",
    "        The eval feeds three vectors of word ids for a, b, c, \n",
    "        each of which is of size N, where N is the number of analogies we want to evaluate in one batch.\n",
    "        \"\"\"\n",
    "        self.analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "        self.analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "        self.analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "\n",
    "        # Normalized word embeddings of shape [vocab_size, emb_dim].\n",
    "        nemb = tf.nn.l2_normalize(self.embed_matrix, 1)\n",
    "\n",
    "        # Each row of a_emb, b_emb, c_emb is a word's embedding vector.\n",
    "        # They all have the shape [N, emb_dim]\n",
    "        a_emb = tf.gather(nemb, self.analogy_a)  # a's embs\n",
    "        b_emb = tf.gather(nemb, self.analogy_b)  # b's embs\n",
    "        c_emb = tf.gather(nemb, self.analogy_c)  # c's embs\n",
    "\n",
    "        # We expect that d's embedding vectors on the unit hyper-sphere is near:\n",
    "        # c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n",
    "        target = c_emb + (b_emb - a_emb)\n",
    "\n",
    "        # Compute cosine distance between each pair of target and vocab.\n",
    "        # dist has shape [N, vocab_size].\n",
    "        dist = tf.matmul(target, nemb, transpose_b=True)\n",
    "\n",
    "        # For each question (row in dist), find the top 20 words.\n",
    "        _, self.pred_idx = tf.nn.top_k(dist, 20)\n",
    "\n",
    "    def predict(self, sess, analogy):\n",
    "        \"\"\" Predict the top 20 answers for analogy questions \"\"\"\n",
    "        idx, = sess.run([self.pred_idx], {\n",
    "            self.analogy_a: analogy[:, 0],\n",
    "            self.analogy_b: analogy[:, 1],\n",
    "            self.analogy_c: analogy[:, 2]\n",
    "        })\n",
    "        return idx\n",
    "\n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_input()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._build_eval_graph()\n",
    "        self._build_nearby_graph()\n",
    "        self._create_summaries()\n",
    "\n",
    "def train_model(sess, model, batch_gen, index_words, num_train_steps):\n",
    "    saver = tf.train.Saver()\n",
    "    # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    initial_step = 0\n",
    "    make_dir('checkpoints') # directory to store checkpoints\n",
    "\n",
    "    sess.run(tf.global_variables_initializer()) # initialize all variables\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    total_loss = 0.0 # use this to calculate late average loss in the last SKIP_STEP steps\n",
    "    writer = tf.summary.FileWriter('graph/lr' + str(model.hps.lr), sess.graph)\n",
    "    initial_step = model.global_step.eval()\n",
    "    for index in range(initial_step, initial_step + num_train_steps):\n",
    "        # feed in new dataset  \n",
    "        if index % model.hps.new_dataset_every == 0:\n",
    "            try:\n",
    "                centers, targets = next(batch_gen)\n",
    "            except StopIteration: # generator has nothing left to generate\n",
    "                batch_gen = get_batch_gen(index_words, \n",
    "                                          model.hps.skip_window, \n",
    "                                          model.hps.num_pairs)\n",
    "                centers, targets = next(batch_gen)\n",
    "                print('Finished looking at the whole text')\n",
    "            \n",
    "            feed = {\n",
    "                model.centers: centers,\n",
    "                model.targets: targets\n",
    "            }\n",
    "            _ = sess.run(model.iterator.initializer, feed_dict = feed)\n",
    "            print('feeding in new dataset')\n",
    "\n",
    "    loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op])\n",
    "    writer.add_summary(summary, global_step=index)\n",
    "    total_loss += loss_batch\n",
    "    if (index + 1) % model.hps.skip_step == 0:\n",
    "        print('Average loss at step {}: {:5.1f}'.format(\n",
    "                                                  index, total_loss/model.hps.skip_step))\n",
    "        total_loss = 0.0\n",
    "        saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "\n",
    "def get_default_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        num_pairs = 10**6,                # number of (center, target) pairs in each dataset instance\n",
    "        vocab_size = 10000,\n",
    "        batch_size = 128,\n",
    "        embed_size = 300,                 # dimension of the word embedding vectors\n",
    "        skip_window = 5,                  # the context window\n",
    "        num_sampled = 100,                # number of negative examples to sample\n",
    "        lr = 0.005,                       # learning rate\n",
    "        new_dataset_every = 10**4,        # replace the original dataset every ? steps\n",
    "        num_train_steps = 8*10**5,        # number of training steps for each feed of dataset\n",
    "        skip_step = 2000\n",
    "    )\n",
    "    return hparams\n",
    "\n",
    "def main():\n",
    "    hps = get_default_hparams()\n",
    "    index_words, dictionary, index_dictionary = process_data(hps.vocab_size)\n",
    "    batch_gen = get_batch_gen(index_words, hps.skip_window, hps.num_pairs)\n",
    "\n",
    "    model = SkipGramModel(hparams = hps)\n",
    "    model.build_graph()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # feed the model with dataset\n",
    "        centers, targets = next(batch_gen)\n",
    "        feed = {\n",
    "            model.centers: centers,\n",
    "            model.targets: targets\n",
    "        }\n",
    "        sess.run(model.iterator.initializer, feed_dict = feed) # initialize the iterator\n",
    "\n",
    "        train_model(sess, model, batch_gen, index_words, hps.num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
