{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "視覺化神經網路讓我們更好地瞭解在這個神秘的巨大網絡中發生了什麼。除了這個應用，Leon Gatys和他的合作者有一個非常有趣的工作，被稱為“神經藝術風格”就是利用神經表徵分離與重組的內容和風格的任意影像算灋，提供了一個藝術形象的創作神經算灋。  \n",
    "結果表明，不同的篩檢程式響應之間的相關性是樣式的表示。很迷人，對吧？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/unahsu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipyd\n",
    "import scipy.misc\n",
    "from libs import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Content Features and Style Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content features:  \n",
    "內容的影像內容特徵的供給內容的影像到神經網路的計算和選取這些content_layers啟動。  \n",
    "Style features:  \n",
    "對於樣式特徵，我們選取樣式影像層的特徵的相關性（矩陣）。通過新增多個層的特徵相關性，我們獲得了輸入影像的多尺度表示，該影像捕獲了紋理資訊，而不是輸入影像中的對象排列。  \n",
    "給定的內容特點和風格特點，我們可以設計一個損失函數，使最終的影像包含的內容，但在風格形象風格的插圖。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss  \n",
    "Our goal is to create an output image which is synthesized by finding an image that simultaneously matches the content features of the photograph and the style features of the respective piece of art. How can we do that? We can define the loss function as the composition of:  \n",
    "1. The dissimilarity of the content features between the output image and the content image; and\n",
    "2. The dissimilarity of the style features between the output image and the style image to the loss function.\n",
    "  \n",
    "The following figure gives a very good visualization of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們從一個嘈雜的初始影像，然後將它作為tensorflow變數，而不是做梯度下降的權重，我們固定的重量和做梯度下降的初始影像來最小化損失函數（這是風格和內容的損失損失的總和）。\n",
    "\n",
    "通過程式碼理解可能更容易。讓我們先從一些偉大的藝術家準備我們最喜歡的內容形象和風格形象。讓我們繼續用神奇的女人作為內容形象，因為她太棒了！對於風格形象，讓我們用梵古的經典作品《星夜》。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "content_directory = 'contents/'\n",
    "style_directory = 'styles/'\n",
    "\n",
    "# This is the directory to store the final stylized images\n",
    "output_directory = 'image_output/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "# This is the directory to store the half-done images during the training.\n",
    "checkpoint_directory = 'checkpoint_output/'\n",
    "if not os.path.exists(checkpoint_directory):\n",
    "    os.makedirs(checkpoint_directory)\n",
    "    \n",
    "content_path = os.path.join(content_directory, 'wonder-woman.jpg')\n",
    "style_path = os.path.join(style_directory, 'starry-night.jpg')\n",
    "output_path = os.path.join(output_directory, 'wonder-woman-starry-night-iteration-1000.jpg')\n",
    "\n",
    "# please notice that the checkpoint_images_path has to contain %s in the file_name\n",
    "checkpoint_path = os.path.join(checkpoint_directory, 'wonder-woman-starry-night-iteration-1000-%s.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_image = utils.imread(content_path)\n",
    "\n",
    "# You can pass several style images as a list, but let's use just one for now.\n",
    "style_images = [utils.imread(style_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for loading the convolution layers of VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "VGG19_LAYERS = (\n",
    "    'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "\n",
    "    'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "\n",
    "\n",
    "    'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "    'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "\n",
    "    'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "    'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "\n",
    "    'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "    'relu5_3', 'conv5_4', 'relu5_4'\n",
    ")\n",
    "\n",
    "\n",
    "def net_preloaded(input_image, pooling):\n",
    "    data_dict = np.load('libs/vgg19.npy', encoding='latin1').item()\n",
    "    net = {}\n",
    "    current = input_image\n",
    "    for i, name in enumerate(VGG19_LAYERS):\n",
    "        kind = name[:4]\n",
    "        if kind == 'conv':\n",
    "            kernels = get_conv_filter(data_dict, name)\n",
    "            # kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
    "\n",
    "            bias = get_bias(data_dict, name)\n",
    "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "\n",
    "            # bias = bias.reshape(-1)\n",
    "            current = conv_layer(current, kernels, bias)\n",
    "        elif kind == 'relu':\n",
    "            current = tf.nn.relu(current)\n",
    "        elif kind == 'pool':\n",
    "            current = pool_layer(current, pooling)\n",
    "\n",
    "        net[name] = current\n",
    "\n",
    "    assert len(net) == len(VGG19_LAYERS)\n",
    "    return net\n",
    "\n",
    "def conv_layer(input, weights, bias):\n",
    "    conv = tf.nn.conv2d(input, weights, strides=(1, 1, 1, 1), padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "\n",
    "def pool_layer(input, pooling):\n",
    "    if pooling == 'avg':\n",
    "        return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                              padding='SAME')\n",
    "    else:\n",
    "        return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "                              padding='SAME')\n",
    "\n",
    "# before we feed the image into the network, we preprocess it by \n",
    "# extracting the mean_pixel from it.\n",
    "def preprocess(image):\n",
    "    return image - VGG_MEAN\n",
    "\n",
    "# remember to unprocess it before you plot it out and save it.\n",
    "def unprocess(image):\n",
    "    return image + VGG_MEAN\n",
    "\n",
    "def get_conv_filter(data_dict, name):\n",
    "    return tf.constant(data_dict[name][0], name=\"filter\")\n",
    "\n",
    "def get_bias(data_dict, name):\n",
    "    return tf.constant(data_dict[name][1], name=\"biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm  \n",
    "This is the main algorithm we will be using to stylize the network. There are a lot of hyper-parameters you can tune. The output image will be stored at output_path, and the checkpoint image (stylized images on every checkpoint_iterations steps) will be stored at checkpoint_path if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "\n",
    "# feel free to try different layers\n",
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "def stylize(content, styles, network_path='libs/imagenet-vgg-verydeep-19.mat', \n",
    "            iterations=1000, content_weight=5e0, content_weight_blend=0.5, style_weight=5e2, \n",
    "            style_layer_weight_exp=1,style_blend_weights=None, tv_weight=100,\n",
    "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "            print_iterations=100, checkpoint_iterations=100, checkpoint_path=None,\n",
    "            output_path=None):\n",
    "\n",
    "\n",
    "    shape = (1,) + content.shape                             #content image shape : (1,433,770,3)\n",
    "    style_shapes = [(1,) + style.shape for style in styles]  #style image shape : (1,600,800,3)\n",
    "    content_features = {}\n",
    "    style_features = [{} for _ in styles]\n",
    "\n",
    "    \n",
    "    # scale the importance of each style layers according to their depth. \n",
    "    # (deeper layers are more important if style_layers_weights > 1 (default = 1))\n",
    "    layer_weight = 1.0\n",
    "    style_layers_weights = {}                                # weight for different network layers\n",
    "    for style_layer in STYLE_LAYERS:                                    \n",
    "        style_layers_weights[style_layer] = layer_weight       #'relu1_1','relu2_1',...,'relu5_1'\n",
    "        layer_weight *= style_layer_weight_exp                 # 1.0\n",
    "\n",
    "\n",
    "    # normalize style layer weights\n",
    "    layer_weights_sum = 0\n",
    "    for style_layer in STYLE_LAYERS:                         #'relu1_1',..., 'relu5_1'\n",
    "        layer_weights_sum += style_layers_weights[style_layer] # 5.0\n",
    "    for style_layer in STYLE_LAYERS:\n",
    "        style_layers_weights[style_layer] /= layer_weights_sum\n",
    "\n",
    "        \n",
    "    # FEATURE MAPS FROM CONTENT IMAGE\n",
    "    # compute the feature map of the content image by feeding it into the network\n",
    "    # the output net contains the features of each content layer\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=shape)\n",
    "\n",
    "        net = net_preloaded(image, pooling)             # {'conv1_1':Tensor,relu1_1:Tensor...}\n",
    "        content_pre = np.array([preprocess(content)])   # (1,433,770,3) subtract the mean pixel\n",
    "        for layer in CONTENT_LAYERS:                    #'relu4_2', 'relu5_2'\n",
    "            content_features[layer] = net[layer].eval(feed_dict={image: content_pre})  \n",
    "\n",
    "            \n",
    "    # FEATURE MAPS (GRAM MATRICES) FROM STYLE IMAGE\n",
    "    # compute style features of the style image by feeding it into the network\n",
    "    # and calculate the gram matrix\n",
    "    for i in range(len(styles)):\n",
    "        g = tf.Graph()\n",
    "        with g.as_default(), tf.Session() as sess:\n",
    "            image = tf.placeholder('float', shape=style_shapes[i])\n",
    "            net = net_preloaded(image, pooling)                           \n",
    "            style_pre = np.array([preprocess(styles[i])])\n",
    "            for layer in STYLE_LAYERS:              #'relu1_1', 'relu2_1',..., 'relu5_1'\n",
    "                features = net[layer].eval(feed_dict={image: style_pre})  # relu_1:(1,600,800,64)\n",
    "                features = np.reshape(features, (-1, features.shape[3]))  # (480000, 64)\n",
    "                gram = np.matmul(features.T, features) / features.size    # (64,64)\n",
    "                style_features[i][layer] = gram\n",
    "\n",
    "                \n",
    "    # make stylized image using backpropogation\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        # Generate a random image (the output image) with the same shape as the content image\n",
    "        initial = tf.random_normal(shape) * 0.256  \n",
    "        image = tf.Variable(initial)\n",
    "        net = net_preloaded(image, pooling)\n",
    "\n",
    "\n",
    "        # CONTENT LOSS\n",
    "        # we can adjust the weight of each content layers\n",
    "        # content_weight_blend is the ratio of two used content layers in this example\n",
    "        content_layers_weights = {}\n",
    "        content_layers_weights['relu4_2'] = content_weight_blend \n",
    "        content_layers_weights['relu5_2'] = 1.0 - content_weight_blend      \n",
    "\n",
    "        content_loss = 0\n",
    "        content_losses = []\n",
    "        for content_layer in CONTENT_LAYERS:\n",
    "            # Use MSE as content losses\n",
    "            # content weight is the coefficient for content loss\n",
    "            content_losses.append(content_layers_weights[content_layer] * content_weight * \n",
    "                                  (2 * tf.nn.l2_loss(net[content_layer] - content_features[content_layer]) /\n",
    "                                   content_features[content_layer].size))\n",
    "        content_loss += reduce(tf.add, content_losses)\n",
    "\n",
    "\n",
    "\n",
    "        # STYLE LOSS\n",
    "        # We can specify different weight for different style images\n",
    "        # style_layers_weights => weight for different network layers\n",
    "        # style_blend_weights => weight between different style images\n",
    "\n",
    "        if style_blend_weights is None:\n",
    "            style_blend_weights = [1.0/len(style_images) for _ in style_images]\n",
    "        else:\n",
    "            total_blend_weight = sum(style_blend_weights)\n",
    "            # normalization\n",
    "            style_blend_weights = [weight/total_blend_weight\n",
    "                                   for weight in style_blend_weights]\n",
    "\n",
    "\n",
    "        style_loss = 0\n",
    "        # iterate to calculate style loss with multiple style images\n",
    "        for i in range(len(styles)):\n",
    "            style_losses = []\n",
    "            for style_layer in STYLE_LAYERS:             # e.g. relu1_1\n",
    "                layer = net[style_layer]                   # relu1_1 of output image:(1,433,770,64)\n",
    "                _, height, width, number = map(lambda i: i.value, layer.get_shape())  \n",
    "                size = height * width * number\n",
    "                feats = tf.reshape(layer, (-1, number))    # (333410,64)\n",
    "\n",
    "                # Gram matrix for the features in relu1_1 of the output image.\n",
    "                gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "                # Gram matrix for the features in relu1_1 of the style image\n",
    "                style_gram = style_features[i][style_layer]   \n",
    "\n",
    "                # Style loss is the MSE for the difference of the 2 Gram matrices\n",
    "                style_losses.append(style_layers_weights[style_layer] * 2 * \n",
    "                                    tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "\n",
    "\n",
    "        # TOTAL VARIATION LOSS  \n",
    "        # Total variation denoising to do smoothing; \n",
    "        # cost to penalize neighboring pixel not used by the original paper by Gatys et al\n",
    "        # According to the paper Mahendran, Aravindh, and Andrea Vedaldi. \n",
    "        # \"Understanding deep image representations by inverting them.\"\n",
    "        # Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\n",
    "        tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "        tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "        tv_loss = tv_weight * 2 * (\n",
    "            (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
    "             tv_y_size) +\n",
    "            (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
    "             tv_x_size))\n",
    "\n",
    "\n",
    "        #OVERALL LOSS\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
    "\n",
    "        def print_progress():\n",
    "            print('     iteration: %d\\n' % i)\n",
    "            print('  content loss: %g\\n' % content_loss.eval())\n",
    "            print('    style loss: %g\\n' % style_loss.eval())\n",
    "            print('       tv loss: %g\\n' % tv_loss.eval())\n",
    "            print('    total loss: %g\\n' % loss.eval())\n",
    "\n",
    "        def imsave(path, img):\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "            Image.fromarray(img).save(path, quality=95)\n",
    "\n",
    "    \n",
    "    \n",
    "        # TRAINING\n",
    "        best_loss = float('inf')\n",
    "        best = None\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            if (print_iterations and print_iterations != 0):\n",
    "                print_progress()\n",
    "\n",
    "            for i in range(iterations):\n",
    "\n",
    "                train_step.run()\n",
    "\n",
    "                last_step = (i == iterations - 1)\n",
    "                if last_step or (print_iterations and i % print_iterations == 0):\n",
    "                    print_progress()\n",
    "\n",
    "                # store output and checkpoint images\n",
    "                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
    "                    this_loss = loss.eval()\n",
    "                    if this_loss < best_loss:\n",
    "                        best_loss = this_loss\n",
    "                        best = image.eval()\n",
    "\n",
    "                    img_out = unprocess(best.reshape(shape[1:]))\n",
    "\n",
    "                    output_file = None\n",
    "                    if not last_step:\n",
    "                        if checkpoint_path:\n",
    "                            output_file = checkpoint_path % i\n",
    "                        else:\n",
    "                            output_file = output_path\n",
    "\n",
    "                        if output_file:\n",
    "                            imsave(output_file, img_out)\n",
    "\n",
    "    print(\"finish stylizing.\")\n",
    "\n",
    "\n",
    "\n",
    "def _tensor_size(tensor):\n",
    "    from operator import mul\n",
    "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iteration: 0\n",
      "\n",
      "  content loss: 383717\n",
      "\n",
      "    style loss: 1.5164e+06\n",
      "\n",
      "       tv loss: 26.2192\n",
      "\n",
      "    total loss: 1.90014e+06\n",
      "\n",
      "     iteration: 0\n",
      "\n",
      "  content loss: 366404\n",
      "\n",
      "    style loss: 1.27638e+06\n",
      "\n",
      "       tv loss: 15739.4\n",
      "\n",
      "    total loss: 1.65852e+06\n",
      "\n",
      "     iteration: 100\n",
      "\n",
      "  content loss: 85520.8\n",
      "\n",
      "    style loss: 26423.2\n",
      "\n",
      "       tv loss: 31910.4\n",
      "\n",
      "    total loss: 143854\n",
      "\n",
      "     iteration: 200\n",
      "\n",
      "  content loss: 77676.3\n",
      "\n",
      "    style loss: 25852\n",
      "\n",
      "       tv loss: 29432.2\n",
      "\n",
      "    total loss: 132961\n",
      "\n",
      "     iteration: 300\n",
      "\n",
      "  content loss: 77106.3\n",
      "\n",
      "    style loss: 25445.8\n",
      "\n",
      "       tv loss: 28503.2\n",
      "\n",
      "    total loss: 131055\n",
      "\n",
      "     iteration: 400\n",
      "\n",
      "  content loss: 78006.9\n",
      "\n",
      "    style loss: 26281.6\n",
      "\n",
      "       tv loss: 28695\n",
      "\n",
      "    total loss: 132983\n",
      "\n",
      "     iteration: 500\n",
      "\n",
      "  content loss: 75787.7\n",
      "\n",
      "    style loss: 26893.5\n",
      "\n",
      "       tv loss: 28061.2\n",
      "\n",
      "    total loss: 130742\n",
      "\n",
      "     iteration: 600\n",
      "\n",
      "  content loss: 75734.1\n",
      "\n",
      "    style loss: 25568\n",
      "\n",
      "       tv loss: 28473.8\n",
      "\n",
      "    total loss: 129776\n",
      "\n",
      "     iteration: 700\n",
      "\n",
      "  content loss: 76611.8\n",
      "\n",
      "    style loss: 23193.7\n",
      "\n",
      "       tv loss: 27849\n",
      "\n",
      "    total loss: 127655\n",
      "\n",
      "     iteration: 800\n",
      "\n",
      "  content loss: 73928.6\n",
      "\n",
      "    style loss: 27342.3\n",
      "\n",
      "       tv loss: 27955.9\n",
      "\n",
      "    total loss: 129227\n",
      "\n",
      "     iteration: 900\n",
      "\n",
      "  content loss: 76229.5\n",
      "\n",
      "    style loss: 23322.4\n",
      "\n",
      "       tv loss: 27684.4\n",
      "\n",
      "    total loss: 127236\n",
      "\n",
      "     iteration: 999\n",
      "\n",
      "  content loss: 72984.1\n",
      "\n",
      "    style loss: 29161\n",
      "\n",
      "       tv loss: 28348.8\n",
      "\n",
      "    total loss: 130494\n",
      "\n",
      "finish stylizing.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path=None\n",
    "output_path='image_output/wonder-woman-starry-night-tvweight-100.jpg'\n",
    "\n",
    "stylize(content_image, style_images, iterations=1000,\n",
    "        content_weight=5e0, content_weight_blend=1, style_weight=5e2, \n",
    "        style_layer_weight_exp=1, style_blend_weights=None, tv_weight=100,\n",
    "        learning_rate=1e1, beta1=0.9, beta2=0.999, epsilon=1e-08, pooling='avg',\n",
    "        print_iterations=100, checkpoint_iterations=100, checkpoint_path=checkpoint_path,\n",
    "        output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
